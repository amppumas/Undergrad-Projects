{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Nombre: Augusto Meza Pe√±a\n",
    "## Tarea 2\n",
    "## Smoothing y Perplejidad de Modelos del lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File correctly processed\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.stem import SnowballStemmer\n",
    "from unicodedata import category\n",
    "from sys import maxunicode\n",
    "from os import walk\n",
    "from nltk.probability import FreqDist\n",
    "from numpy import zeros as npZeros, nditer as npIterator, set_printoptions, where as npWhere, isclose as npIsClose\n",
    "from math import log2, isclose\n",
    "from collections import defaultdict\n",
    "from fractions import Fraction\n",
    "\n",
    "\n",
    "def vocab_dict():\n",
    "    vocab = defaultdict()\n",
    "    vocab.default_factory = lambda: len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def text2numba(documents, vocab):\n",
    "    for doc in documents:\n",
    "        yield [vocab[w] for w in doc]\n",
    "\n",
    "# Save all Unicode characters for punctuation in a list\n",
    "punctuation = [char for char in range(maxunicode) if category(chr(char)).startswith('P')]\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self, path, encoding, input_type, lam=1):\n",
    "        \"\"\"\n",
    "        Class that process a Corpus and obtains useful information\n",
    "        :param path: path to the file or directory to process\n",
    "        :param encoding: string with the encoding\n",
    "        :param input_type: either 'file' or 'directory' depending on path\n",
    "        :param lam: lambda for the model, defaults to laplace smoothing\n",
    "        \"\"\"\n",
    "        self.encoding = encoding\n",
    "        self.text = ''  # files are small so we can save them in a String\n",
    "        self.stems = dict()\n",
    "        self.tokens = None\n",
    "        self.lam = lam\n",
    "        if input_type == 'file':\n",
    "            self.source = path\n",
    "            self._retrieve_single_file()\n",
    "        else:\n",
    "            self.directory_path = path\n",
    "            self._retrieve_texts_directory()\n",
    "\n",
    "        self._clean_and_tokenize()\n",
    "        self._obtain_stems()\n",
    "        self._getFrequencies()\n",
    "        print(\"File correctly processed\")\n",
    "\n",
    "    def _retrieve_single_file(self, filename=''):\n",
    "        filename = (self.directory_path + '/' + filename) if hasattr(self, 'directory_path') else self.source\n",
    "        with open(filename, encoding=self.encoding) as currentfile:\n",
    "            for line in currentfile:\n",
    "                self.text += line\n",
    "\n",
    "    def _retrieve_texts_directory(self):\n",
    "        \"\"\" Obtaings all the files in the given directory and attempts to join them to the corpus\"\"\"\n",
    "        filenames = self._files_from_directory()\n",
    "        for filename in filenames:\n",
    "            self._retrieve_single_file(filename)\n",
    "\n",
    "    def _files_from_directory(self):\n",
    "        \"\"\"Crawls to only the root folder looking for any files\"\"\"\n",
    "        f = []\n",
    "        for (dirpath, dirnames, filenames) in walk(self.directory_path):\n",
    "            f.extend(filenames)\n",
    "            break\n",
    "        return f\n",
    "\n",
    "    def _clean_and_tokenize(self):\n",
    "        no_punctuation = self.text.translate(dict.fromkeys(punctuation))\n",
    "        self.tokens = word_tokenize(no_punctuation.lower())\n",
    "\n",
    "    def _obtain_stems(self):\n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "        self.types = set(self.tokens)\n",
    "        for token in self.types:\n",
    "            self.stems.update({token: stemmer.stem(token)})\n",
    "\n",
    "    def _getFrequencies(self):\n",
    "        \"\"\"Retrieves the text from all the files, strips punctuation and sets lowercase,\n",
    "           then tokenizes the String and obtains the types\n",
    "           after that ir obtains the stem of all the types\"\"\"\n",
    "        stemmed_text = [self.stems[token] for token in self.tokens]\n",
    "\n",
    "        self.bigram_freq = BigramCollocationFinder.from_words(stemmed_text).ngram_fd\n",
    "        self.stem_freq = FreqDist(stemmed_token for stemmed_token in stemmed_text)\n",
    "\n",
    "        self.stem2id = vocab_dict()\n",
    "        self.stem_in_position = list(text2numba([stemmed_text], self.stem2id))\n",
    "        self.id2stem = list(self.stem2id.keys())\n",
    "        self.vocab_length = len(self.stem2id)\n",
    "        self.first_stem = self.id2stem[self.stem_in_position[0][0]]\n",
    "        self.last_stem_id = self.stem_in_position[0][-1]\n",
    "        self._getModel()\n",
    "\n",
    "    def _getModel(self, vocab_length=None, id2stem=None, lam=None):\n",
    "        vocab_length = self.vocab_length if vocab_length is None else vocab_length  # reduce verbosity\n",
    "        id2stem = self.id2stem if id2stem is None else id2stem\n",
    "        lam = self.lam if lam is None else lam\n",
    "\n",
    "        tran_prob_matrix = npZeros((vocab_length, vocab_length))  # fill bigram probabilities matrix\n",
    "        elements = npIterator(tran_prob_matrix, op_flags=['writeonly'], flags=['multi_index'])\n",
    "        while not elements.finished:\n",
    "            i, j = elements.multi_index\n",
    "            wi, wj = id2stem[i], id2stem[j]\n",
    "            fr_wi_and_wj = self.bigram_freq[(wj, wi)]\n",
    "            elements[0] = (fr_wi_and_wj + lam) / (self.stem_freq[wj] + lam * vocab_length)\n",
    "            elements.iternext()\n",
    "\n",
    "        tran_prob_sum = tran_prob_matrix.sum(axis=0)  # check for consistency\n",
    "        if len(npWhere(npIsClose(tran_prob_sum, [1.0]) is False)) != 1:\n",
    "            print(\"Sum of columns:\", tran_prob_sum)\n",
    "            print(\"Error: Probability matrix of bigrams is not coherent, ones != 1\")\n",
    "            exit(1)\n",
    "        elif tran_prob_sum.argmin() != self.last_stem_id:\n",
    "            print(\"Sum of columns:\", tran_prob_sum)\n",
    "            print(\"Word with the minimum:\", id2stem[tran_prob_sum.argmin()])\n",
    "            print(\"Error: Probability matrix of bigrams is not coherent, min is not last word\")\n",
    "            exit(1)\n",
    "\n",
    "        init_prob_matrix = npZeros((1, vocab_length))  # fill initial probabilities matrix\n",
    "        elements = npIterator(init_prob_matrix, op_flags=['writeonly'], flags=['f_index'])\n",
    "        while not elements.finished:\n",
    "            x = elements.index\n",
    "            wi = id2stem[x]\n",
    "            fr_wi_initial = 1 if wi == self.first_stem else 0\n",
    "            elements[0] = (fr_wi_initial + lam) / (1 + lam * vocab_length)\n",
    "            elements.iternext()\n",
    "\n",
    "        if not isclose(init_prob_matrix.sum(), 1.0):  # check consistency\n",
    "            print(\"Sum of row:\", init_prob_matrix.sum())\n",
    "            print(\"Error: Probability matrix of initials is not coherent, sum is not close to 1\")\n",
    "            exit(1)\n",
    "\n",
    "        perplex = 0\n",
    "        for x in range(vocab_length):\n",
    "            perplex += log2(init_prob_matrix[0, x])\n",
    "            for y in range(vocab_length):\n",
    "                perplex += log2(tran_prob_matrix[x, y])\n",
    "        self.perplex = perplex / (-1 * vocab_length)\n",
    "\n",
    "        self.tran_prob_matrix = tran_prob_matrix\n",
    "        self.init_prob_matrix = init_prob_matrix\n",
    "\n",
    "    def get_perplexity(self, newlam=None):\n",
    "        if newlam is not None and newlam != self.lam:\n",
    "            self.lam = newlam\n",
    "            self._getModel()\n",
    "        return \"The perplexity of the model with lambda '\"+str(self.lam)+\"' is of 2 to the power of \"+str(self.perplex), (self.lam, self.perplex)\n",
    "\n",
    "    def bigram_prob(self, word_i, word_j):\n",
    "        initial = self.init_prob_matrix[0, word_i]\n",
    "        bigram = self.tran_prob_matrix[word_i, word_j]\n",
    "        return initial * bigram\n",
    "\n",
    "    def numTokens(self):\n",
    "        return 'The corpus has '+str(len(self.tokens))+' tokens'\n",
    "\n",
    "    def numTypes(self):\n",
    "        return 'The corpus has '+str(len(self.types))+' types'\n",
    "\n",
    "corpus = Corpus(\"corpus_bigramas.txt\", 'utf-8', input_type=\"file\")\n",
    "set_printoptions(formatter={'all': lambda x: str(Fraction(x).limit_denominator())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplejidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the model with lambda '1' is of 2 to the power of 4161.697183628841\n",
      "The perplexity of the model with lambda '0.5' is of 2 to the power of 4164.205491632786\n",
      "The perplexity of the model with lambda '0.1' is of 2 to the power of 4187.906301465618\n",
      "The perplexity of the model with lambda '0.01' is of 2 to the power of 4389.293799688266\n"
     ]
    }
   ],
   "source": [
    "for lambd in [1, 0.5, 0.1, 0.01]:\n",
    "    print(corpus.get_perplexity(lambd)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
